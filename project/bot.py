import logging
import requests
import asyncio
import torch
import re
from collections import Counter
import matplotlib.pyplot as plt
from aiogram import Bot, Dispatcher, F
from aiogram.types import Message, ReplyKeyboardMarkup, KeyboardButton, FSInputFile
from bs4 import BeautifulSoup
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.nn.functional import softmax
from pymorphy2 import MorphAnalyzer
from g4f.client import Client


TOKEN = "8098528206:AAGGaTbY4kz3f9FYPf-2LP5EE4c2D1SWLKY"

bot = Bot(token=TOKEN)
dp = Dispatcher()
logging.basicConfig(level=logging.INFO)

keyboard = ReplyKeyboardMarkup(
    keyboard=[[KeyboardButton(text="üìä –ü–æ–ª—É—á–∏—Ç—å –∞–Ω–∞–ª–∏–∑")]],
    resize_keyboard=True
)

MODEL_NAME = "blanchefort/rubert-base-cased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)
morph = MorphAnalyzer()

STOP_WORDS = {
    "–æ—á–µ–Ω—å", "—Ç–æ–ª—å–∫–æ", "–µ—Å–ª–∏", "–µ—Å—Ç—å", "–±—ã—Ç—å", "–ø—Ä–æ—Å—Ç–æ", "–±—ã–ª", "—Ç–∞–∫", "—ç—Ç–æ", "–ø–æ—Å–ª–µ", "—á–µ—Ä–µ–∑", "–Ω–∞–º", "–µ–≥–æ", "–æ–Ω–∞",
    "—Ç–∞–∫–∂–µ", "–µ—â—ë", "–≤—Å—ë", "—Å–∞–º—ã–π", "—Ö–æ—Ä–æ—à–æ", "–µ—â—ë", "—Ä–∞–∑", "–Ω—É", "–ø–∞—Ä—É", "–º–æ–∂–Ω–æ", "–º–æ–≥—É—Ç", "–∫–∞–∂–¥—ã–π", "–∫–∞–∫–æ–π", "–∫–∞–∫–∞—è",
    "–ª–∏—à—å", "–¥–æ–ª–∂–µ–Ω", "–º–æ–≥—É", "–º–æ—á—å", "–±–æ–ª–µ–µ", "–º–µ–Ω–µ–µ", "–≥–¥–µ", "–∫—É–¥–∞", "–∫–æ–≥–¥–∞", "—Å–Ω–æ–≤–∞", "–∏–º–µ–Ω–Ω–æ", "–≤—Å–µ–≥–¥–∞", "–Ω–∏–∫–æ–≥–¥–∞",
    "—É–∂–µ", "—Ç–µ–ø–µ—Ä—å", "—Å–æ–≤—Å–µ–º", "–ø–æ—á—Ç–∏", "–Ω—É–∂–Ω–æ", "–ª—É—á—à–µ", "–≤–æ–æ–±—â–µ", "–∫–æ–Ω–µ—á–Ω–æ", "–ø–æ–∂–∞–ª—É–π—Å—Ç–∞", "—Å–ø–∞—Å–∏–±–æ", "–æ–¥–∏–Ω", "–¥–≤–∞",
}

client = Client()


def clean_text(text):
    text = re.sub(r'[^\w\s,.!?—ë–Å–∞-—è–ê-–Ø]', '', text).strip().lower()
    return text


def get_reviews(url):
    response = requests.get(url)
    if response.status_code != 200:
        return ["–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Å—ã–ª–∫—É."]

    soup = BeautifulSoup(response.text, "html.parser")
    reviews = [clean_text(r.text) for r in soup.findAll('span', class_='business-review-view__body-text')]
    return reviews if reviews else ["–ù–∞ –¥–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ—Ç –æ—Ç–∑—ã–≤–æ–≤."]


def normalize_word(word):
    return morph.parse(word)[0].normal_form


def delete_stop_words(text):
    words = [normalize_word(word) for word in text.split() if word not in STOP_WORDS and len(word) > 3]
    return words


def analyze_sentiment(reviews):
    results = {"negative": 0, "neutral": 0, "positive": 0}
    details = []
    keywords = {"negative": [], "positive": []}

    for review in reviews:
        inputs = tokenizer(review, return_tensors="pt", truncation=True, padding=True, max_length=512)
        outputs = model(**inputs)
        probabilities = softmax(outputs.logits, dim=-1)
        sentiment = torch.argmax(probabilities).item()
        sentiment_label = ["negative", "neutral", "positive"][sentiment]
        results[sentiment_label] += 1
        details.append({"text": review, "sentiment": sentiment_label, "probabilities": probabilities.tolist()})
        if sentiment_label in ["negative", "positive"]:
            keywords[sentiment_label].extend(delete_stop_words(review))
    return results, details, keywords


def save_analysis_to_file(details):
    file_path = "sentiment_analysis_results.txt"
    with open(file_path, "w", encoding="utf-8") as f:
        for d in details:
            f.write(f"–û—Ç–∑—ã–≤: {d['text']}\n")
            f.write(f"–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {d['sentiment']}\n")
            f.write(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: {d['probabilities']}\n\n")
    return file_path


def generate_recommendation_with_gpt(common_words):
    if not common_words:
        return "–ù–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π."

    prompt = (f"–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–º—É –±–∏–∑–Ω–µ—Å—É. –î–∞–π –∫—Ä–∞—Ç–∫—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –ø–æ —É–ª—É—á—à–µ–Ω–∏—é —Å–µ—Ä–≤–∏—Å–∞ –∑–∞–≤–µ–¥–µ–Ω–∏—è –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–∏—Ç–∞–Ω–∏—è –Ω–∞ "
              f"–æ—Å–Ω–æ–≤–µ –æ—Ç–∑—ã–≤–æ–≤ –∫–ª–∏–µ–Ω—Ç–æ–≤. –£–ø–æ–º—è–Ω–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –∞ –∏–º–µ–Ω–Ω–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –º–µ—Å—Ç–∞, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ "
              f"—á–∞—â–µ –≤—Å–µ–≥–æ –∂–∞–ª—É—é—Ç—Å—è –∫–ª–∏–µ–Ω—Ç—ã: {', '.join(common_words)}. "
              f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω–æ–π, –Ω–æ –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–π. –¢–≤–æ—è —Ü–µ–ª—å - –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ "
              f"–ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Ç–æ—á–∫–∏, —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å –æ –Ω–∏—Ö –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ, –∞ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç —É–ª—É—á—à–µ–Ω–∏—è"
              f"–¥–ª—è –∏–∑–±–∞–≤–ª–µ–Ω–∏—è –æ—Ç –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –º–µ—Å—Ç. –ù–∞–ø–∏—à–∏ —Ç–æ–ª—å–∫–æ –ö–†–ê–¢–ö–ò–ô –æ—Ç–≤–µ—Ç –≤ –ù–ï–°–ö–û–õ–¨–ö–û –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ô! –ù–ï –ò–°–ü–û–õ–¨–ó–£–ô"
              f"–≠–ú–û–î–ñ–ò –ò –ñ–ò–†–ù–´–ô –®–†–ò–§–¢!")
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        web_search=False
    )
    return response.choices[0].message.content


def summary_reviews_with_gpt(common_words):
    if not common_words:
        return "–ù–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π."

    prompt = (f"–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–º—É –±–∏–∑–Ω–µ—Å—É. –î–∞–π –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, "
              f"–∞ –∏–º–µ–Ω–Ω–æ —Ç–µ—Ö –≤–µ—â–µ–π, –æ –∫–æ—Ç–æ—Ä—ã—Ö –ª—é–¥–∏ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–ª–∏–∫–∞–ª–∏—Å—å —á–∞—â–µ –≤—Å–µ–≥–æ –ø—Ä–∏ –ø–æ—Å–µ—â–µ–Ω–∏–∏ –∑–∞–≤–µ–¥–µ–Ω–∏—è: "
              f"{', '.join(common_words)}. –û—Ç—á–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã–º –∏ –∫—Ä–∞—Ç–∫–∏–º, —Ç–≤–æ—è –∑–∞–¥–∞—á–∞ - –æ–±–æ–±—â–∏—Ç—å —Ö–æ—Ä–æ—à–∏–µ –º–æ–º–µ–Ω—Ç—ã "
              f"–∏ –¥–∞—Ç–æ—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫—Ä–∞—Ç–∫—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–µ–π –ø–æ –¥–æ–ø —É–ª—É—á—à–µ–Ω–∏—è–º. –ù–∞–ø–∏—à–∏ —Ç–æ–ª—å–∫–æ –ö–†–ê–¢–ö–ò–ô "
              f"–æ—Ç–≤–µ—Ç –≤ –ù–ï–°–ö–û–õ–¨–ö–û –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ô! –ù–ï –ò–°–ü–û–õ–¨–ó–£–ô –≠–ú–û–î–ñ–ò –ò –ñ–ò–†–ù–´–ô –®–†–ò–§–¢!")
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        web_search=False
    )
    return response.choices[0].message.content


def generate_recommendations(keywords):
    recommendations = {}
    for sentiment, words in keywords.items():
        common_words = [word for word, _ in Counter(words).most_common(5)]
        if sentiment == "negative":
            recommendations[sentiment] = {
                "words": common_words,
                "recommendation": generate_recommendation_with_gpt(common_words)
            }
        else:
            recommendations[sentiment] = {
                "words": common_words,
                "recommendation": summary_reviews_with_gpt(common_words)
            }
    return recommendations


def visualize_distribution(results):
    plt.figure(figsize=(8, 6))
    plt.bar(results.keys(), results.values(), color=['red', 'yellow', 'green'])
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–∑—ã–≤–æ–≤')
    plt.xlabel('–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏')
    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤')
    plt.xticks(["negative", "neutral", "positive"], ["–ù–µ–≥–∞—Ç–∏–≤", "–ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ", "–ü–æ–∑–∏—Ç–∏–≤"])
    plt.grid(axis='y', alpha=0.7)
    plt.savefig("sentiment_distribution.png")
    plt.close()


@dp.message(F.text == "/start")
async def send_welcome(message: Message):
    await message.answer(
        "üëã –ü—Ä–∏–≤–µ—Ç! –Ø –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é –æ—Ç–∑—ã–≤—ã —Å –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç –∏ –¥–∞—é –≥—Ä–∞–º–æ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏. \n"
        "–ß—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –∞–Ω–∞–ª–∏–∑, –Ω–∞–∂–º–∏—Ç–µ –Ω–∞ –∫–Ω–æ–ø–∫—É –Ω–∏–∂–µ",
        reply_markup=keyboard
    )


@dp.message(F.text == "üìä –ü–æ–ª—É—á–∏—Ç—å –∞–Ω–∞–ª–∏–∑")
async def ask_for_link(message: Message):
    await message.answer("–û—Ç–ø—Ä–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ –∑–∞–≤–µ–¥–µ–Ω–∏–µ –≤ –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç–∞—Ö.")


@dp.message()
async def process_reviews(message: Message):
    url = message.text.strip()
    if "yandex.ru/maps" not in url:
        await message.reply("‚ùå –û—à–∏–±–∫–∞ –≤ URL. –î–∞–Ω–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç–∞–º.")
        return
    await message.reply("‚è≥ –°–æ–±–∏—Ä–∞—é –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é –æ—Ç–∑—ã–≤—ã...")
    reviews = get_reviews(url)
    if len(reviews) == 1 and "–û—à–∏–±–∫–∞" in reviews[0]:
        await message.reply(reviews[0])
        return
    results, details, keywords = analyze_sentiment(reviews)
    recommendations = generate_recommendations(keywords)
    visualize_distribution(results)

    rec_text = "\n\n".join([
        f"‚û°Ô∏è {s.upper()}:\n–°–ª–æ–≤–∞: {', '.join(recommendations[s]['words'])}\n"
        f"{recommendations[s]['recommendation']}"
        for s in recommendations
    ])

    summary = (f"–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω!\n"
               f"üî¥ –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {results['negative']}\n"
               f"üü° –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö: {results['neutral']}\n"
               f"üü¢ –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö: {results['positive']}\n\n\n\n"
               f"{rec_text}")

    await message.answer_photo(FSInputFile("sentiment_distribution.png"), caption="üìä –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—Ç–∑—ã–≤–æ–≤")
    await message.answer(summary)
    report_file = save_analysis_to_file(details)
    await message.answer_document(FSInputFile(report_file), caption="–ü–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–æ–≤")


async def main():
    await bot.delete_webhook(drop_pending_updates=True)
    await dp.start_polling(bot)


if __name__ == "__main__":
    asyncio.run(main())
